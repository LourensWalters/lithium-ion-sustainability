import pickle
import numpy as np
import pandas as pd
from src.data.data_class import BatteryData
import src.constants as cst

EXTERNAL_DATA_DIR = cst.EXTERNAL_DATA_DIR


class DataLoader:
    """
    Load data from source into both a dataframe and dictionary. The source of data is the AWS Data Lake. The
    class can however also read from other sources e.g. datafiles generated by the 'Long Live the Battery' project
    application.
    TODO: Implement design pattern such as Factory Method for loading data...
    TODO: Implement an object to store data, based on a design pattern for inheritance. Something like:
    GenericDataModel->MLModel->SupervisedModel or GenericDataModel->ReportingModel->BIDashboardModel etc.
    """

    def __init__(self, model_id, battery_data, grain=None, stage=None) -> object:
        """
        Initialises DataLoader object.
        :param model_id:
        :param grain:
        :param stage:
        """
        self._model_id = model_id
        self._grain = grain
        self._stage = stage
        self._data = battery_data

    def read_data_from_source(self):
        """"
        Load data from source into both a dataframe and dictionary. The source of data is the AWS Data Lake. The
        class can however also read from other sources e.g. datafiles generated by the 'Long Live the Battery' project
        application.
        TODO: A factory design pattern or something else that is relevant should be implemented here.
        TODO: The Long Live the Battery project matlab files should be read and processed.
        TODO: The Long Live the Battery code should be tested via unit tests.
        TODO: The Long Live the Battery code write to output files in consistent manner with other use cases.
        :rtype: pandas.DataFrame
        """
        # TODO: Add code to read data from .mat files, currently only reading from pickle files.
        self._data.data_dict = self._load_batches_to_dict()
        self._data.data_frame = pd.DataFrame(self._data.data_dict)

    def read_data_from_pickle(self, processed_data_path):
        """
        Load previously processed data from pickled file. Return dataframe.
        :param processed_data_path:
        :return: pandas .DataFrame
        """
        # Read the pickle file
        picklefile = open(processed_data_path, 'rb')
        # Unpickle the dataframe
        self._data.data_frame = pickle.load(picklefile)
        # Close file
        picklefile.close()

    @staticmethod
    def write_data_to_pickle(df, processed_data_path):
        """
        Write processed data passed as parameter to pickle file in specified directory.
        :param df:
        :param processed_data_path:
        :return:
        """
        # Create a pickle file
        picklefile = open(processed_data_path, 'wb')
        # Pickle the dataframe
        pickle.dump(df, picklefile)
        # Close file
        picklefile.close()

    @staticmethod
    def write_data_to_source(df, table_name):
        """
        Write processed data to Data Lake.
        :param df:
        :param table_name:
        :return:
        """
        pass

    def _read_nature_data(self):
        pass

    def _load_batches_to_dict(self, amount_to_load=3):
        """Loads batches from disc and returns one concatenated dict.
        amount_to_load specifies the number of batches to load, starting from 1."""
        if amount_to_load < 1 or amount_to_load > 3:
            raise Exception("amount_to_load is not a valid number! Try a number between 1 and 3.")

        batches_dict = {}  # Initializing

        # Replicating Load Data logic
        print("Loading batch1 ...")
        # path1 = join(DATA_DIR, '/batch1.pkl')
        path1 = EXTERNAL_DATA_DIR + "/batch1.pkl"
        batch1 = pickle.load(open(path1, 'rb'))

        # remove batteries that do not reach 80% capacity
        del batch1['b1c8']
        del batch1['b1c10']
        del batch1['b1c12']
        del batch1['b1c13']
        del batch1['b1c22']

        batches_dict.update(batch1)

        if amount_to_load > 1:
            print("Loading batch2 ...")
            # path2 = join(DATA_DIR, "/batch2.pkl")
            path2 = EXTERNAL_DATA_DIR + "/batch2.pkl"
            batch2 = pickle.load(open(path2, 'rb'))

            # There are four cells from batch1 that carried into batch2, we'll remove the data from batch2
            # and put it with the correct cell from batch1
            batch2_keys = ['b2c7', 'b2c8', 'b2c9', 'b2c15', 'b2c16']
            batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4']
            add_len = [662, 981, 1060, 208, 482]

            for i, bk in enumerate(batch1_keys):
                batch1[bk]['cycle_life'] = batch1[bk]['cycle_life'] + add_len[i]
                for j in batch1[bk]['summary'].keys():
                    if j == 'cycle':
                        batch1[bk]['summary'][j] = np.hstack((batch1[bk]['summary'][j],
                                                              batch2[batch2_keys[i]]['summary'][j] + len(
                                                                  batch1[bk]['summary'][j])))
                    else:
                        batch1[bk]['summary'][j] = np.hstack(
                            (batch1[bk]['summary'][j], batch2[batch2_keys[i]]['summary'][j]))
                last_cycle = len(batch1[bk]['cycles'].keys())
                for j, jk in enumerate(batch2[batch2_keys[i]]['cycles'].keys()):
                    batch1[bk]['cycles'][str(last_cycle + j)] = batch2[batch2_keys[i]]['cycles'][jk]

            del batch2['b2c7']
            del batch2['b2c8']
            del batch2['b2c9']
            del batch2['b2c15']
            del batch2['b2c16']

            # All keys have to be updated after the reordering.
            batches_dict.update(batch1)
            batches_dict.update(batch2)

        if amount_to_load > 2:
            print("Loading batch3 ...")
            # path3 = join(DATA_DIR, "batch3.pkl")
            path3 = EXTERNAL_DATA_DIR + "/batch3.pkl"
            batch3 = pickle.load(open(path3, 'rb'))

            # remove noisy channels from batch3
            del batch3['b3c37']
            del batch3['b3c2']
            del batch3['b3c23']
            del batch3['b3c32']
            del batch3['b3c38']
            del batch3['b3c39']

            batches_dict.update(batch3)

        print("Done loading batches")
        return batches_dict

    def get_dictionary(self) -> object:
        """
        Getter method for data dictionary.
        :return: Loaded data dictionary.
        """
        return self._data.data_dict

    # TODO: Remove hardcoded file references
    def save_preprocessed_data(self, results_dict, save_dir='./data/interim/processed_data.pkl'):
        print("Saving preprocessed data to {}".format(save_dir))
        with open(save_dir, 'wb') as f:
            pickle.dump(results_dict, f)


    def load_preprocessed_data(self, save_dir='./data/interim/processed_data.pkl'):
        print("Loading preprocessed data from {}".format(save_dir))
        with open(save_dir, 'rb') as f:
            return pickle.load(f)


def main():
    """
    Class tester.
    """
    battery_data = BatteryData()
    data_loader = DataLoader("long_live", battery_data)
    # Reading data from AWS MySQL database TODO: generalise this.
    data_loader.read_data_from_source()


if __name__ == "__main__":
    # Call main for testing.
    main()
