{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battery life cycle prediction - deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import src.constants as cst\n",
    "import src.features.rebuilding_features as rf\n",
    "import src.models.train_model as tm\n",
    "#from src.data.data_class import BatteryData\n",
    "#from src.data.load_data import DataLoader\n",
    "#from src.data.wrangle_data import DataWrangler\n",
    "\n",
    "#from rebuilding_features import load_batches_to_dict\n",
    "from src.visualization.helpers import print_dict_keys\n",
    "from os.path import join\n",
    "import src.models.data_pipeline as dp\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "import itertools\n",
    "import warnings\n",
    "import json\n",
    "import seaborn as sns\n",
    "import src.constants as cst\n",
    "\n",
    "import src.models.data_pipeline as dp  # TODO: Have to refactor this code out of this class.\n",
    "import src.models.split_model as split_model\n",
    "import src.models.full_cnn_model as full_cnn_model\n",
    "from src.models.callbacks import CustomCheckpoints\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "#session = tf.compat.v1.Session(config=config)\n",
    "#tf.compat.v1.keras.backend.set_session(session)\n",
    "import os\n",
    "from pandas import datetime\n",
    "\n",
    "DATA_DIR = join(\"../../data/external\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print (physical_devices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def calculate_steps_per_epoch(data_dir, dataset_config):\n",
    "    temp_dataset = dp.create_dataset(data_dir=data_dir,\n",
    "                                     window_size=dataset_config[\"window_size\"],\n",
    "                                     shift=dataset_config[\"shift\"],\n",
    "                                     stride=dataset_config[\"stride\"],\n",
    "                                     batch_size=dataset_config[\"batch_size\"],\n",
    "                                     repeat=False)\n",
    "    steps_per_epoch = 0\n",
    "    for batch in temp_dataset:\n",
    "        steps_per_epoch += 1\n",
    "    return steps_per_epoch\n",
    "\n",
    "def get_tboard_dir():\n",
    "    run_timestr = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tboard_dir = os.path.join(cst.TENSORBOARD_DIR, \"jobs\", run_timestr)\n",
    "    return tboard_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We build a deep learning model by\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "args = tm.get_args()\n",
    "#model_trainer = tm.ModelTrainer(None, args)\n",
    "#model_trainer.train_and_evaluate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph\\jobs\\20230226-060056\n",
      "Using split model!\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Qdlin (InputLayer)             [(None, 20, 1000, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Tdlin (InputLayer)             [(None, 20, 1000, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " detail_concat (Concatenate)    (None, 20, 1000, 2)  0           ['Qdlin[0][0]',                  \n",
      "                                                                  'Tdlin[0][0]']                  \n",
      "                                                                                                  \n",
      " convolution (TimeDistributed)  (None, 20, 334, 32)  608         ['detail_concat[0][0]']          \n",
      "                                                                                                  \n",
      " conv_pool (TimeDistributed)    (None, 20, 167, 32)  0           ['convolution[0][0]']            \n",
      "                                                                                                  \n",
      " conv2 (TimeDistributed)        (None, 20, 56, 64)   18496       ['conv_pool[0][0]']              \n",
      "                                                                                                  \n",
      " pool2 (TimeDistributed)        (None, 20, 28, 64)   0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (TimeDistributed)        (None, 20, 10, 128)  73856       ['pool2[0][0]']                  \n",
      "                                                                                                  \n",
      " pool3 (TimeDistributed)        (None, 20, 5, 128)   0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " convolution_flat (TimeDistribu  (None, 20, 640)     0           ['pool3[0][0]']                  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_cnn (TimeDistributed)  (None, 20, 640)      0           ['convolution_flat[0][0]']       \n",
      "                                                                                                  \n",
      " IR (InputLayer)                [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Discharge_time (InputLayer)    [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " QD (InputLayer)                [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " all_concat (Concatenate)       (None, 20, 643)      0           ['dropout_cnn[0][0]',            \n",
      "                                                                  'IR[0][0]',                     \n",
      "                                                                  'Discharge_time[0][0]',         \n",
      "                                                                  'QD[0][0]']                     \n",
      "                                                                                                  \n",
      " recurrent (LSTM)               (None, 128)          395264      ['all_concat[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_lstm (Dropout)         (None, 128)          0           ['recurrent[0][0]']              \n",
      "                                                                                                  \n",
      " hidden (Dense)                 (None, 32)           4128        ['dropout_lstm[0][0]']           \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            66          ['hidden[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 492,418\n",
      "Trainable params: 492,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n"
     ]
    }
   ],
   "source": [
    "scaling_factors = dp.load_scaling_factors()\n",
    "dataset_dir = cst.TEST_SET\n",
    "window_size = 20\n",
    "shift = 5\n",
    "stride = 1\n",
    "batch_size = 32\n",
    "hparams = None\n",
    "save_from = 80\n",
    "tboard = get_tboard_dir()\n",
    "\n",
    "print (tboard)\n",
    "\n",
    "dataset = dp.create_dataset(dataset_dir,\n",
    "                            window_size=window_size,\n",
    "                            shift=shift,  # Can vary during validation\n",
    "                            stride=stride,\n",
    "                            batch_size=batch_size,  # Can vary during validation\n",
    "                            cycle_length=1,  # To match original order (so no files get interleaved)\n",
    "                            num_parallel_calls=1,  # Has to be equal or below cycle_length\n",
    "                            shuffle=None,  # To match original order\n",
    "                            repeat=None)\n",
    "\n",
    "# Config datasets for consistent usage\n",
    "ds_config = dict(window_size=window_size,\n",
    "                 shift=shift,\n",
    "                 stride=stride,\n",
    "                 batch_size=batch_size)\n",
    "ds_train_path = cst.TRAIN_SET\n",
    "ds_val_path = cst.TEST_SET\n",
    "\n",
    "# create model\n",
    "if args.model == 'split_model':\n",
    "    print(\"Using split model!\")\n",
    "    model = split_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                           loss=args.loss,\n",
    "                                           hparams_config=hparams)\n",
    "if args.model == 'full_cnn_model':\n",
    "    print(\"Using full cnn model!\")\n",
    "    model = full_cnn_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                              loss=args.loss,\n",
    "                                              hparams_config=hparams)\n",
    "\n",
    "# Calculate steps_per_epoch_train, steps_per_epoch_test\n",
    "# This is needed, since for counting repeat has to be false\n",
    "steps_per_epoch_train = calculate_steps_per_epoch(ds_train_path, ds_config)\n",
    "\n",
    "steps_per_epoch_validate = calculate_steps_per_epoch(ds_val_path, ds_config)\n",
    "\n",
    "# load datasets\n",
    "dataset_train = dp.create_dataset(data_dir=ds_train_path,\n",
    "                                  window_size=ds_config[\"window_size\"],\n",
    "                                  shift=ds_config[\"shift\"],\n",
    "                                  stride=ds_config[\"stride\"],\n",
    "                                  batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "dataset_validate = dp.create_dataset(data_dir=ds_val_path,\n",
    "                                     window_size=ds_config[\"window_size\"],\n",
    "                                     shift=ds_config[\"shift\"],\n",
    "                                     stride=ds_config[\"stride\"],\n",
    "                                     batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "# if hparams is passed, we're running a HPO-job\n",
    "if hparams:\n",
    "    checkpoint_callback = CustomCheckpoints(save_last_only=True,\n",
    "                                            log_dir=tboard,\n",
    "                                            dataset_path=ds_val_path,\n",
    "                                            dataset_config=ds_config,\n",
    "                                            save_eval_plot=False)\n",
    "else:\n",
    "    checkpoint_callback = CustomCheckpoints(save_best_only=True,\n",
    "                                            start_epoch=save_from,\n",
    "                                            log_dir=tboard,\n",
    "                                            dataset_path=ds_val_path,\n",
    "                                            dataset_config=ds_config,\n",
    "                                            save_eval_plot=False)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=tboard,\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False,\n",
    "                                   ),\n",
    "    checkpoint_callback,\n",
    "]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=500,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    validation_data=dataset_validate,\n",
    "    validation_steps=steps_per_epoch_validate,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "mae_current = min(history.history[\"val_mae_current_cycle\"])\n",
    "mae_remaining = min(history.history[\"val_mae_remaining_cycles\"])\n",
    "\n",
    "#return mae_current, mae_remaining"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mae_current = min(history.history[\"val_mae_current_cycle\"])\n",
    "mae_remaining = min(history.history[\"val_mae_remaining_cycles\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(10, 10))\n",
    "line1, = axes.plot(history.epoch, history.history['loss'], label='train')\n",
    "line2, = axes.plot(history.epoch, history.history['val_loss'], label='validation')\n",
    "_ = axes.xaxis.set_label_text(\"Epoch\")\n",
    "_ = axes.yaxis.set_label_text(\"Loss\")\n",
    "_ = axes.legend(handles=[line1, line2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(10, 10))\n",
    "line1, = axes.plot(history.epoch, history.history['mae_remaining_cycles'], label='train')\n",
    "line2, = axes.plot(history.epoch, history.history['val_mae_remaining_cycles'], label='validation')\n",
    "_ = axes.xaxis.set_label_text(\"Epoch\")\n",
    "_ = axes.yaxis.set_label_text(\"MAE\")\n",
    "_ = axes.legend(handles=[line1, line2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
